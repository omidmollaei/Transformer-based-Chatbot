{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2hWmQVmZIEI"
      },
      "source": [
        "<h3><b>Train and evaluate a chatbot based on an encoder-decoder transformer model ( i.e. same as the original transformer model ) . The model is trained on the Cornell-Movie-Dialog dataset.</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHTHq6OmaGPL"
      },
      "source": [
        "<h5><b> 0. Setup</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eLCWHnocZHkO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from dataset import DatasetHp, preprocess_sentence, get_cornell_dataset\n",
        "from transformer_model import ModelHp, encoder_decoder_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPhFN0nacJrP"
      },
      "source": [
        "<h5><b> 1. Load dataset and tokenizer</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7lGHJvrZHhv",
        "outputId": "e8528543-e296-4e79-bb30-5bd03d510ed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9916637/9916637 [==============================] - 1s 0us/step\n",
            "loading conversations ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:14<00:00, 5849.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initializing tokenizer ...\n",
            "tokenizer saved in `./transformer/tokenizer`\n",
            "vocab size updated from 10000 to 10054\n",
            "tokenization ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "221616it [00:14, 14881.07it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset_hp = DatasetHp(\n",
        "    max_length = 128,\n",
        "    vocab_size = 10_000,\n",
        "    max_sample=None,\n",
        ")\n",
        "\n",
        "dataset, tokenizer = get_cornell_dataset(dataset_hp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6zpmKc8vWvx"
      },
      "source": [
        "<h5><b> 2. Define loss and metric functions.</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_LVrG_ibZHfi"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "def loss_fn(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, dataset_hp.max_length - 1))\n",
        "    loss = cross_entropy(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, dataset_hp.max_length - 1))\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CRutE4_xCOQ"
      },
      "source": [
        "<h5><b> 3. Build and train the model.</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUFqlEGhZHdV",
        "outputId": "5b6a98f7-c40d-464b-c6ec-cdfd29d272b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of model's parameters: 36481862\n"
          ]
        }
      ],
      "source": [
        "hparams = ModelHp(\n",
        "    d_model = 512,\n",
        "    num_attention_heads = 8,\n",
        "    dropout_rate = 0.1,\n",
        "    num_units = 1024,\n",
        "    activation = \"relu\",\n",
        "    vocab_size = 10054,\n",
        "    num_layers = 4,\n",
        ")\n",
        "\n",
        "model = encoder_decoder_transformer(hparams, \"transformer\")\n",
        "print(f\"Total number of model's parameters: {model.count_params()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TafdtRT2wbpD"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[accuracy])\n",
        "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"model_checkpoint_cb.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2J9BPv8jla4",
        "outputId": "35b6b83c-536e-4aba-ed83-695cbdd05afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "6902/6902 [==============================] - 2099s 298ms/step - loss: 0.7456 - accuracy: 0.0092\n",
            "Epoch 2/5\n",
            "6902/6902 [==============================] - 2020s 293ms/step - loss: 0.7395 - accuracy: 0.0095\n",
            "Epoch 3/5\n",
            "6902/6902 [==============================] - 2016s 292ms/step - loss: 0.7379 - accuracy: 0.0096\n",
            "Epoch 4/5\n",
            "6902/6902 [==============================] - 2019s 292ms/step - loss: 0.7374 - accuracy: 0.0096\n",
            "Epoch 5/5\n",
            "6902/6902 [==============================] - 2016s 292ms/step - loss: 0.7373 - accuracy: 0.0096\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=5, callbacks=[model_checkpoint_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0boGEXMxVBc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
