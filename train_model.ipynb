{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2hWmQVmZIEI"
      },
      "source": [
        "<h3><b>Train and evaluate a chatbot based on an encoder-decoder transformer model ( i.e. same as the original transformer model ) . The model is trained on the Cornell-Movie-Dialog dataset.</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHTHq6OmaGPL"
      },
      "source": [
        "<h5><b> 0. Setup</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eLCWHnocZHkO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from transformer_model import PositionalEncoding, MultiHeadAttentionLayer\n",
        "from dataset import DatasetHp, preprocess_sentence, get_cornell_dataset\n",
        "from transformer_model import ModelHp, encoder_decoder_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPhFN0nacJrP"
      },
      "source": [
        "<h5><b> 1. Load dataset and tokenizer</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7lGHJvrZHhv",
        "outputId": "e63452c4-08a3-4ed8-b0cb-5c8375fd4735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9916637/9916637 [==============================] - 1s 0us/step\n",
            "loading conversations ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 18638/83097 [00:03<00:12, 5029.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initializing tokenizer ...\n",
            "tokenizer saved in `./transformer/tokenizer`\n",
            "vocab size updated from 10000 to 10255\n",
            "tokenization ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50000it [00:03, 14348.49it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset_hp = DatasetHp(\n",
        "    max_length = 40,\n",
        "    vocab_size = 10_000,\n",
        "    max_sample=50_000,\n",
        ")\n",
        "\n",
        "dataset, tokenizer = get_cornell_dataset(dataset_hp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6zpmKc8vWvx"
      },
      "source": [
        "<h5><b> 2. Define loss and metric functions.</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_LVrG_ibZHfi"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, dataset_hp.max_length - 1))\n",
        "    loss = cross_entropy(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, dataset_hp.max_length - 1))\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CRutE4_xCOQ"
      },
      "source": [
        "<h5><b> 3. Build and train the model.</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUFqlEGhZHdV",
        "outputId": "8de0a75b-1727-46c2-f74a-109d8d2423bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of model's parameters: 10521871\n"
          ]
        }
      ],
      "source": [
        "hparams = ModelHp(\n",
        "    d_model = 256,\n",
        "    num_attention_heads = 8,\n",
        "    dropout_rate = 0.1,\n",
        "    num_units = 512,\n",
        "    activation = \"relu\",\n",
        "    vocab_size = 10255,\n",
        "    num_layers = 2,\n",
        ")\n",
        "\n",
        "model = encoder_decoder_transformer(hparams, \"transformer\")\n",
        "\n",
        "print(f\"Total number of model's parameters: {model.count_params()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TafdtRT2wbpD"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2J9BPv8jla4",
        "outputId": "ecaea5c9-55a1-4463-98c1-93c525c57e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1382/1382 [==============================] - 134s 80ms/step - loss: 0.3908 - accuracy: 0.2244\n",
            "Epoch 2/50\n",
            "1382/1382 [==============================] - 70s 51ms/step - loss: 0.3884 - accuracy: 0.2251\n",
            "Epoch 3/50\n",
            "1382/1382 [==============================] - 67s 48ms/step - loss: 0.3865 - accuracy: 0.2255\n",
            "Epoch 4/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3860 - accuracy: 0.2253\n",
            "Epoch 5/50\n",
            "1382/1382 [==============================] - 66s 47ms/step - loss: 0.3896 - accuracy: 0.2246\n",
            "Epoch 6/50\n",
            "1382/1382 [==============================] - 67s 48ms/step - loss: 0.3844 - accuracy: 0.2259\n",
            "Epoch 7/50\n",
            "1382/1382 [==============================] - 65s 47ms/step - loss: 0.3855 - accuracy: 0.2255\n",
            "Epoch 8/50\n",
            "1382/1382 [==============================] - 65s 47ms/step - loss: 0.3860 - accuracy: 0.2254\n",
            "Epoch 9/50\n",
            "1382/1382 [==============================] - 65s 47ms/step - loss: 0.3838 - accuracy: 0.2262\n",
            "Epoch 10/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3839 - accuracy: 0.2259\n",
            "Epoch 11/50\n",
            "1382/1382 [==============================] - 65s 47ms/step - loss: 0.3849 - accuracy: 0.2257\n",
            "Epoch 12/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3829 - accuracy: 0.2260\n",
            "Epoch 13/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3859 - accuracy: 0.2254\n",
            "Epoch 14/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3822 - accuracy: 0.2262\n",
            "Epoch 15/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3821 - accuracy: 0.2263\n",
            "Epoch 16/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.3845 - accuracy: 0.2256\n",
            "Epoch 17/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3833 - accuracy: 0.2259\n",
            "Epoch 18/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3827 - accuracy: 0.2262\n",
            "Epoch 19/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3826 - accuracy: 0.2261\n",
            "Epoch 20/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3827 - accuracy: 0.2262\n",
            "Epoch 21/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3816 - accuracy: 0.2263\n",
            "Epoch 22/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3821 - accuracy: 0.2263\n",
            "Epoch 23/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3849 - accuracy: 0.2254\n",
            "Epoch 24/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3823 - accuracy: 0.2263\n",
            "Epoch 25/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.3834 - accuracy: 0.2259\n",
            "Epoch 26/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3833 - accuracy: 0.2260\n",
            "Epoch 27/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3821 - accuracy: 0.2263\n",
            "Epoch 28/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3818 - accuracy: 0.2263\n",
            "Epoch 29/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3806 - accuracy: 0.2266\n",
            "Epoch 30/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3806 - accuracy: 0.2265\n",
            "Epoch 31/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3814 - accuracy: 0.2264\n",
            "Epoch 32/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3825 - accuracy: 0.2262\n",
            "Epoch 33/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3833 - accuracy: 0.2259\n",
            "Epoch 34/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.3794 - accuracy: 0.2268\n",
            "Epoch 35/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3801 - accuracy: 0.2268\n",
            "Epoch 36/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3804 - accuracy: 0.2267\n",
            "Epoch 37/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.3790 - accuracy: 0.2272\n",
            "Epoch 38/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3780 - accuracy: 0.2271\n",
            "Epoch 39/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.3772 - accuracy: 0.2276\n",
            "Epoch 40/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3776 - accuracy: 0.2274\n",
            "Epoch 41/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3778 - accuracy: 0.2271\n",
            "Epoch 42/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3770 - accuracy: 0.2273\n",
            "Epoch 43/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3774 - accuracy: 0.2273\n",
            "Epoch 44/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.3762 - accuracy: 0.2276\n",
            "Epoch 45/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.3762 - accuracy: 0.2278\n",
            "Epoch 46/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3760 - accuracy: 0.2278\n",
            "Epoch 47/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.3760 - accuracy: 0.2278\n",
            "Epoch 48/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.3764 - accuracy: 0.2275\n",
            "Epoch 49/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3748 - accuracy: 0.2279\n",
            "Epoch 50/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.3751 - accuracy: 0.2278\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S0boGEXMxVBc"
      },
      "outputs": [],
      "source": [
        "def inference(hp, model, tokenizer, sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    sentence = tf.expand_dims(\n",
        "        hp.start_token + tokenizer.encode(sentence) + hp.end_token, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims(hp.start_token, 0)\n",
        "\n",
        "    for i in range(hp.max_length):\n",
        "        predictions = model(inputs=[sentence, output], training=False)\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if tf.equal(predicted_id, hp.end_token[0]):\n",
        "            break\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    \n",
        "    return tf.squeeze(output, axis=0)\n",
        "\n",
        "def generate_response(hp, model, tokenizer, sentence):\n",
        "    prediction = inference(hp, model, tokenizer, sentence)\n",
        "    predicted_sentence = tokenizer.decode(\n",
        "        [i for i in prediction if i < tokenizer.vocab_size]\n",
        "    )\n",
        "    return predicted_sentence\n",
        "\n",
        "def evaluate(hp, model, tokenizer, inputs):\n",
        "    print(\"-evaluating ...\")\n",
        "    response = \"what are you going to do?\"\n",
        "\n",
        "    for user_sentnece in inputs:\n",
        "        if user_sentnece != None:\n",
        "            print(f\"\\nInput: {user_sentnece}\")\n",
        "            response = generate_response(hp, model, tokenizer, user_sentnece)\n",
        "            print(f\"Output: {response}\")\n",
        "        \n",
        "        else:\n",
        "            print(f\"\\nInput: {response}\")\n",
        "            response = generate_response(hp, model, tokenizer, response)\n",
        "            print(f\"Output: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bn6DDilmhSGu"
      },
      "outputs": [],
      "source": [
        "from dataset import load_tokenizer\n",
        "\n",
        "tokenizer = load_tokenizer(\"./transformer/tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ol7LvkRxhSD-"
      },
      "outputs": [],
      "source": [
        "chatbot = tf.keras.models.load_model(\n",
        "    \"model.h5\",\n",
        "    custom_objects={\n",
        "            \"PositionalEncoding\": PositionalEncoding,\n",
        "            \"MultiHeadAttentionLayer\": MultiHeadAttentionLayer,\n",
        "        },\n",
        "    compile=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAi4QrYFhSBX",
        "outputId": "bc69c916-2618-40e9-ff9e-1a9c0d04eace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-evaluating ...\n",
            "\n",
            "Input: What is your name ?\n",
            "Output: i do not know . i am going to do .\n",
            "\n",
            "Input: I want to be a good programmer\n",
            "Output: i am sorry .\n",
            "\n",
            "Input: Do you love winter or summer ?\n",
            "Output: i do not know . i am not sure . i have not in the mood .\n",
            "\n",
            "Input: Tomarrow, i have an important exam\n",
            "Output: i am sorry , but can i do not gamble , hilda !\n",
            "\n",
            "Input: what is your age\n",
            "Output: i am sorry , i do not know .\n",
            "\n",
            "Input: i am sorry , i do not know .\n",
            "Output: i am not convinced you should be happy .\n",
            "\n",
            "Input: i am not convinced you should be happy .\n",
            "Output: i do not know . it is a maze of tunnels . i cannot see it .\n",
            "\n",
            "Input: i do not know . it is a maze of tunnels . i cannot see it .\n",
            "Output: i do not want to .\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"What is your name ?\",\n",
        "    \"I want to be a good programmer\",\n",
        "    \"Do you love winter or summer ?\",\n",
        "    \"Tomarrow, i have an important exam\",\n",
        "    \"what is your age\",\n",
        "    None, None, None\n",
        "]\n",
        "\n",
        "evaluate(dataset_hp, chatbot, tokenizer, sentences)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
