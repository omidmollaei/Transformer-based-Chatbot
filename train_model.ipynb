{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2hWmQVmZIEI"
      },
      "source": [
        "<h3><b>Train and evaluate a chatbot based on an encoder-decoder transformer model ( i.e. same as the original transformer model ) . The model is trained on the Cornell-Movie-Dialog dataset.</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHTHq6OmaGPL"
      },
      "source": [
        "<h5><b> 0. Setup</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eLCWHnocZHkO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from transformer_model import PositionalEncoding, MultiHeadAttentionLayer\n",
        "from dataset import DatasetHp, preprocess_sentence, get_cornell_dataset\n",
        "from transformer_model import ModelHp, encoder_decoder_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPhFN0nacJrP"
      },
      "source": [
        "<h5><b> 1. Load dataset and tokenizer</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7lGHJvrZHhv",
        "outputId": "0940057d-6b47-4a58-f439-a5035b4613e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9916637/9916637 [==============================] - 2s 0us/step\n",
            "loading conversations ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 18638/83097 [00:02<00:09, 6920.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initializing tokenizer ...\n",
            "tokenizer saved in `./transformer/tokenizer`\n",
            "vocab size updated from 10000 to 10255\n",
            "tokenization ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50000it [00:02, 19004.71it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset_hp = DatasetHp(\n",
        "    max_length = 40,\n",
        "    vocab_size = 10_000,\n",
        "    max_sample=50_000,\n",
        ")\n",
        "\n",
        "dataset, tokenizer = get_cornell_dataset(dataset_hp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6zpmKc8vWvx"
      },
      "source": [
        "<h5><b> 2. Define loss and metric functions.</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_LVrG_ibZHfi"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, dataset_hp.max_length - 1))\n",
        "    loss = cross_entropy(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, dataset_hp.max_length - 1))\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CRutE4_xCOQ"
      },
      "source": [
        "<h5><b> 3. Build and train the model.</b></h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUFqlEGhZHdV",
        "outputId": "3f4fb1a5-0f55-4ebd-ef1c-e9b12d2b696f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of model's parameters: 10521871\n"
          ]
        }
      ],
      "source": [
        "hparams = ModelHp(\n",
        "    d_model = 256,\n",
        "    num_attention_heads = 8,\n",
        "    dropout_rate = 0.1,\n",
        "    num_units = 512,\n",
        "    activation = \"relu\",\n",
        "    vocab_size = 10255,\n",
        "    num_layers = 2,\n",
        ")\n",
        "\n",
        "#model = encoder_decoder_transformer(hparams, \"transformer\")\n",
        "\n",
        "model = tf.keras.models.load_model(\n",
        "    \"model_checkpoint_cb.h5\",\n",
        "    custom_objects={\"PositionalEncoding\":PositionalEncoding,\n",
        "                    \"MultiHeadAttentionLayer\": MultiHeadAttentionLayer},\n",
        "    compile= False)\n",
        "print(f\"Total number of model's parameters: {model.count_params()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TafdtRT2wbpD"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"model_checkpoint_cb.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2J9BPv8jla4",
        "outputId": "94d8e797-6134-4468-9016-be525073472d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4689 - accuracy: 0.2060\n",
            "Epoch 2/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.4680 - accuracy: 0.2060\n",
            "Epoch 3/50\n",
            "1382/1382 [==============================] - 64s 46ms/step - loss: 0.4666 - accuracy: 0.2062\n",
            "Epoch 4/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4664 - accuracy: 0.2064\n",
            "Epoch 5/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4660 - accuracy: 0.2065\n",
            "Epoch 6/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4649 - accuracy: 0.2066\n",
            "Epoch 7/50\n",
            "1382/1382 [==============================] - 61s 44ms/step - loss: 0.4641 - accuracy: 0.2071\n",
            "Epoch 8/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4632 - accuracy: 0.2073\n",
            "Epoch 9/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.4626 - accuracy: 0.2072\n",
            "Epoch 10/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4613 - accuracy: 0.2077\n",
            "Epoch 11/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4598 - accuracy: 0.2081\n",
            "Epoch 12/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4596 - accuracy: 0.2082\n",
            "Epoch 13/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4604 - accuracy: 0.2077\n",
            "Epoch 14/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4589 - accuracy: 0.2084\n",
            "Epoch 15/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4584 - accuracy: 0.2084\n",
            "Epoch 16/50\n",
            "1382/1382 [==============================] - 61s 44ms/step - loss: 0.4575 - accuracy: 0.2085\n",
            "Epoch 17/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4569 - accuracy: 0.2087\n",
            "Epoch 18/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4555 - accuracy: 0.2089\n",
            "Epoch 19/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4545 - accuracy: 0.2091\n",
            "Epoch 20/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4530 - accuracy: 0.2096\n",
            "Epoch 21/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4527 - accuracy: 0.2095\n",
            "Epoch 22/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.4521 - accuracy: 0.2099\n",
            "Epoch 23/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4516 - accuracy: 0.2099\n",
            "Epoch 24/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4505 - accuracy: 0.2100\n",
            "Epoch 25/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.4492 - accuracy: 0.2105\n",
            "Epoch 26/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4493 - accuracy: 0.2104\n",
            "Epoch 27/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.4496 - accuracy: 0.2104\n",
            "Epoch 28/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4481 - accuracy: 0.2107\n",
            "Epoch 29/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.4477 - accuracy: 0.2109\n",
            "Epoch 30/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4472 - accuracy: 0.2110\n",
            "Epoch 31/50\n",
            "1382/1382 [==============================] - 62s 44ms/step - loss: 0.4469 - accuracy: 0.2107\n",
            "Epoch 32/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4456 - accuracy: 0.2113\n",
            "Epoch 33/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4444 - accuracy: 0.2117\n",
            "Epoch 34/50\n",
            "1382/1382 [==============================] - 61s 44ms/step - loss: 0.4430 - accuracy: 0.2120\n",
            "Epoch 35/50\n",
            "1382/1382 [==============================] - 62s 44ms/step - loss: 0.4434 - accuracy: 0.2119\n",
            "Epoch 36/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4417 - accuracy: 0.2123\n",
            "Epoch 37/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4408 - accuracy: 0.2126\n",
            "Epoch 38/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4414 - accuracy: 0.2124\n",
            "Epoch 39/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4403 - accuracy: 0.2127\n",
            "Epoch 40/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4405 - accuracy: 0.2126\n",
            "Epoch 41/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4390 - accuracy: 0.2130\n",
            "Epoch 42/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4384 - accuracy: 0.2131\n",
            "Epoch 43/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4389 - accuracy: 0.2130\n",
            "Epoch 44/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4379 - accuracy: 0.2132\n",
            "Epoch 45/50\n",
            "1382/1382 [==============================] - 63s 45ms/step - loss: 0.4374 - accuracy: 0.2134\n",
            "Epoch 46/50\n",
            "1382/1382 [==============================] - 63s 46ms/step - loss: 0.4388 - accuracy: 0.2128\n",
            "Epoch 47/50\n",
            "1382/1382 [==============================] - 62s 44ms/step - loss: 0.4367 - accuracy: 0.2136\n",
            "Epoch 48/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4385 - accuracy: 0.2133\n",
            "Epoch 49/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4365 - accuracy: 0.2135\n",
            "Epoch 50/50\n",
            "1382/1382 [==============================] - 62s 45ms/step - loss: 0.4344 - accuracy: 0.2140\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=50, callbacks=[model_checkpoint_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S0boGEXMxVBc"
      },
      "outputs": [],
      "source": [
        "def inference(hp, model, tokenizer, sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    sentence = tf.expand_dims(\n",
        "        hp.start_token + tokenizer.encode(sentence) + hp.end_token, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims(hp.start_token, 0)\n",
        "\n",
        "    for i in range(hp.max_length):\n",
        "        predictions = model(inputs=[sentence, output], training=False)\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if tf.equal(predicted_id, hp.end_token[0]):\n",
        "            break\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    \n",
        "    return tf.squeeze(output, axis=0)\n",
        "\n",
        "def generate_response(hp, model, tokenizer, sentence):\n",
        "    prediction = inference(hp, model, tokenizer, sentence)\n",
        "    predicted_sentence = tokenizer.decode(\n",
        "        [i for i in prediction if i < tokenizer.vocab_size]\n",
        "    )\n",
        "    return predicted_sentence\n",
        "\n",
        "def evaluate(hp, model, tokenizer, inputs):\n",
        "    print(\"-evaluating ...\")\n",
        "    response = \"what are you going to do?\"\n",
        "\n",
        "    for user_sentnece in inputs:\n",
        "        if user_sentnece != None:\n",
        "            print(f\"\\nInput: {user_sentnece}\")\n",
        "            response = generate_response(hp, model, tokenizer, user_sentnece)\n",
        "            print(f\"Output: {response}\")\n",
        "        \n",
        "        else:\n",
        "            print(f\"\\nInput: {response}\")\n",
        "            response = generate_response(hp, model, tokenizer, response)\n",
        "            print(f\"Output: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Bn6DDilmhSGu"
      },
      "outputs": [],
      "source": [
        "from dataset import load_tokenizer\n",
        "\n",
        "tokenizer = load_tokenizer(\"./transformer/tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ol7LvkRxhSD-"
      },
      "outputs": [],
      "source": [
        "chatbot = tf.keras.models.load_model(\n",
        "    \"model_checkpoint_cb.h5\",\n",
        "    custom_objects={\n",
        "            \"PositionalEncoding\": PositionalEncoding,\n",
        "            \"MultiHeadAttentionLayer\": MultiHeadAttentionLayer,\n",
        "        },\n",
        "    compile=False\n",
        ")\n",
        "\n",
        "sentences = [\n",
        "    \"Hello, my name is Omid . what about you?\",\n",
        "    \"how was your day\",\n",
        "    \"How old are you?\",\n",
        "    \"where have you been\",\n",
        "    \"do you like pizza?\",\n",
        "    \"my favourite color is blue\",\n",
        "    None, None, None\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAi4QrYFhSBX",
        "outputId": "11df3f34-7d7c-4781-d89d-035658a0ca7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-evaluating ...\n",
            "\n",
            "Input: Hello, my name is Omid . what about you?\n",
            "Output: i am not sure working , i guess . . .\n",
            "\n",
            "Input: how was your day\n",
            "Output: i am sorry , honey . i am not going to need some time .\n",
            "\n",
            "Input: How old are you?\n",
            "Output: i do not know . i am just having some people you to say children need a new ones . but at least they do with me , do with me , do the middle of the teens .\n",
            "\n",
            "Input: where have you been\n",
            "Output: i am not going anywhere . i am here . i am right here . i will not let s get you .\n",
            "\n",
            "Input: do you like pizza?\n",
            "Output: i am not sure . i am betting on you .\n",
            "\n",
            "Input: my favourite color is blue\n",
            "Output: i am not going anywhere . i have got to do not like you .\n",
            "\n",
            "Input: i am not going anywhere . i have got to do not like you .\n",
            "Output: i am not going anywhere . i am right here . i win .\n",
            "\n",
            "Input: i am not going anywhere . i am right here . i win .\n",
            "Output: i am not going to cover for this . it change .\n",
            "\n",
            "Input: i am not going to cover for this . it change .\n",
            "Output: i am not going anywhere . i am right here . i win .\n"
          ]
        }
      ],
      "source": [
        "evaluate(dataset_hp, chatbot, tokenizer, sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38_I5eNkhR8A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
