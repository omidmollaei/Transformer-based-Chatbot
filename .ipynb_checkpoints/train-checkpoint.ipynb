{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "OIlsEd19HhpL",
   "metadata": {
    "id": "OIlsEd19HhpL"
   },
   "source": [
    "<p><b>1. imports</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fyqW3hdN6yOT",
   "metadata": {
    "executionInfo": {
     "elapsed": 5195,
     "status": "ok",
     "timestamp": 1667543101318,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "fyqW3hdN6yOT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from transformer import model\n",
    "from transformer.dataset import HyperParameters\n",
    "from transformer.dataset import get_dataset, preprocess_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_R5wLP-CHmOX",
   "metadata": {
    "id": "_R5wLP-CHmOX"
   },
   "source": [
    "<p><b>2.1 Define a custom learning-rate scheduler.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "MwZ6a5Oj6yOZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1667543105109,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "MwZ6a5Oj6yOZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model: int, warmup_steps: int = 4000):\n",
    "        super(LearningRateSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * self.warmup_steps**-1.5\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hS6CC6LwIs7L",
   "metadata": {
    "id": "hS6CC6LwIs7L"
   },
   "source": [
    "<p><b>2.2 Define hyper-parameters.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wGrS9D5MI2L7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1667543107642,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "wGrS9D5MI2L7",
    "outputId": "6de7c638-e1a4-443c-dce2-1526a1c99ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sample : 50000\n",
      "max_length : 40\n",
      "batch_size : 32\n",
      "num_layers : 2\n",
      "num_units : 512\n",
      "d_model : 256\n",
      "num_heads : 8\n",
      "dropout : 0.1\n",
      "activation : relu\n"
     ]
    }
   ],
   "source": [
    "hparams = HyperParameters(\n",
    "    max_sample=50000,\n",
    "    max_length=40,\n",
    "    batch_size=32,\n",
    "    num_layers=2,\n",
    "    num_units=512,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    dropout=0.1,\n",
    "    activation=\"relu\",\n",
    ")\n",
    "\n",
    "for property, value in vars(hparams).items():\n",
    "    print(property, \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lLtQZKz_JD9f",
   "metadata": {
    "id": "lLtQZKz_JD9f"
   },
   "source": [
    "<p><b>3. Load dataset and tokenizer.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uihTFrMRI_tV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54430,
     "status": "ok",
     "timestamp": 1667543166681,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "uihTFrMRI_tV",
    "outputId": "6b3379a8-ace5-42af-9a2f-bda391ae24c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
      "9916637/9916637 [==============================] - 1s 0us/step\n",
      "loading conversations ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 18638/83097 [00:03<00:12, 5116.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing tokenizer ...\n",
      "tokenization ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:03, 13671.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# load dataset and tokenizer(then save tokenizer too)\n",
    "dataset, tokenizer = get_dataset(hparams)\n",
    "\n",
    "# define a function to load pretrained tokenizer\n",
    "def load_tokenizer(path=\"./transformer/tokenizer\"):\n",
    "    tokenizer = tfds.deprecated.text.SubwordTextEncoder.load_from_file(path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ApwZ7zJRKimb",
   "metadata": {
    "id": "ApwZ7zJRKimb"
   },
   "source": [
    "<p><b>4. Define loss, optimizer and metric(s).</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "jDY0ls1rKiG7",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667543166682,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "jDY0ls1rKiG7"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "        LearningRateSchedule(d_model=hparams.d_model), beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    ")\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, hparams.max_length - 1))\n",
    "    loss = cross_entropy(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, hparams.max_length - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RiaUqtRqIAZf",
   "metadata": {
    "id": "RiaUqtRqIAZf"
   },
   "source": [
    "<p><b>4. Build and train the model.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jzxN2Ph4HaAo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4308,
     "status": "ok",
     "timestamp": 1667543306385,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "jzxN2Ph4HaAo",
    "outputId": "1ed61499-211f-498f-cce7-863efd32a828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-model built\n"
     ]
    }
   ],
   "source": [
    "# build new model\n",
    "tf.keras.utils.set_random_seed(1234)\n",
    "chatbot_model = model.transformer(hparams)\n",
    "\n",
    "# load pretrained model : for fine tuning\n",
    "#chatbot_model = tf.keras.models.load_model(\n",
    "#    \"model.h5\",\n",
    "#    custom_objects={\n",
    "#            \"PositionalEncoding\": model.PositionalEncoding,\n",
    "#            \"MultiHeadAttentionLayer\": model.MultiHeadAttentionLayer,\n",
    "#        },\n",
    "#    compile=False\n",
    "#)\n",
    "print(\"-model built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "COt94K4dHZ-A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1238801,
     "status": "ok",
     "timestamp": 1667547787244,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "COt94K4dHZ-A",
    "outputId": "54f43ddf-70b5-4ab2-ec0a-f1b4c622da04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start\n",
      "Epoch 1/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5653 - accuracy: 0.1988\n",
      "Epoch 2/20\n",
      "1374/1374 [==============================] - 57s 42ms/step - loss: 0.5624 - accuracy: 0.1995\n",
      "Epoch 3/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5615 - accuracy: 0.1996\n",
      "Epoch 4/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5587 - accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "1374/1374 [==============================] - 57s 41ms/step - loss: 0.5572 - accuracy: 0.2005\n",
      "Epoch 6/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5551 - accuracy: 0.2012\n",
      "Epoch 7/20\n",
      "1374/1374 [==============================] - 56s 41ms/step - loss: 0.5536 - accuracy: 0.2015\n",
      "Epoch 8/20\n",
      "1374/1374 [==============================] - 54s 40ms/step - loss: 0.5519 - accuracy: 0.2021\n",
      "Epoch 9/20\n",
      "1374/1374 [==============================] - 54s 40ms/step - loss: 0.5497 - accuracy: 0.2024\n",
      "Epoch 10/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5486 - accuracy: 0.2027\n",
      "Epoch 11/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5459 - accuracy: 0.2030\n",
      "Epoch 12/20\n",
      "1374/1374 [==============================] - 57s 41ms/step - loss: 0.5449 - accuracy: 0.2036\n",
      "Epoch 13/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5437 - accuracy: 0.2038\n",
      "Epoch 14/20\n",
      "1374/1374 [==============================] - 57s 41ms/step - loss: 0.5414 - accuracy: 0.2042\n",
      "Epoch 15/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5401 - accuracy: 0.2044\n",
      "Epoch 16/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5385 - accuracy: 0.2053\n",
      "Epoch 17/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5368 - accuracy: 0.2054\n",
      "Epoch 18/20\n",
      "1374/1374 [==============================] - 57s 41ms/step - loss: 0.5355 - accuracy: 0.2056\n",
      "Epoch 19/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5331 - accuracy: 0.2061\n",
      "Epoch 20/20\n",
      "1374/1374 [==============================] - 55s 40ms/step - loss: 0.5329 - accuracy: 0.2063\n"
     ]
    }
   ],
   "source": [
    "chatbot_model.compile(optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "print(\"train start\")\n",
    "history = chatbot_model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6B6IhWoiL9ZL",
   "metadata": {
    "id": "6B6IhWoiL9ZL"
   },
   "source": [
    "<p><b>5. Save the model.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gPkHRxf7HZ7r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1667547822001,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "gPkHRxf7HZ7r",
    "outputId": "1ee8ed57-a925-422b-e48c-a82c934d0884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-saving Model to: model.h5\n"
     ]
    }
   ],
   "source": [
    "saving_model = \"model.h5\"\n",
    "hparams.save_model = saving_model\n",
    "print(f\"-saving Model to: {saving_model}\")\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "        chatbot_model, filepath=hparams.save_model, include_optimizer=False)\n",
    "\n",
    "\n",
    "with open('hyper-parameters.pkl', 'wb') as hp:\n",
    "    pickle.dump(hparams, hp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bBuJ4xQTNAjC",
   "metadata": {
    "id": "bBuJ4xQTNAjC"
   },
   "source": [
    "<p><b>6. Evaluate the Model.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "-0MiOhosSYpf",
   "metadata": {
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1667547827497,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "-0MiOhosSYpf"
   },
   "outputs": [],
   "source": [
    "def inference(hp, model, tokenizer, sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    sentence = tf.expand_dims(\n",
    "        hparams.start_token + tokenizer.encode(sentence) + hparams.end_token, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims(hparams.start_token, 0)\n",
    "\n",
    "    for i in range(hparams.max_length):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if tf.equal(predicted_id, hparams.end_token[0]):\n",
    "            break\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "    \n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "def generate_response(hp, model, tokenizer, sentence):\n",
    "    prediction = inference(hp, model, tokenizer, sentence)\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "    return predicted_sentence\n",
    "\n",
    "def evaluate(hparams, model, tokenizer, inputs):\n",
    "    print(\"-evaluating ...\")\n",
    "    response = \"what are you going to do?\"\n",
    "\n",
    "    for user_sentnece in inputs:\n",
    "        if user_sentnece != None:\n",
    "            print(f\"\\nInput: {user_sentnece}\")\n",
    "            response = generate_response(hparams, model, tokenizer, user_sentnece)\n",
    "            print(f\"Output: {response}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"\\nInput: {response}\")\n",
    "            response = generate_response(hparams, model, tokenizer, response)\n",
    "            print(f\"Output: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1AmEW7dqHZ5l",
   "metadata": {
    "executionInfo": {
     "elapsed": 7482,
     "status": "ok",
     "timestamp": 1667547842123,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "1AmEW7dqHZ5l"
   },
   "outputs": [],
   "source": [
    "# loading saved componentes (model, hyper-parameters, tokenizer)\n",
    "with open('hyper-parameters.pkl', 'rb') as hp:\n",
    "    hparams_loaded = pickle.load(hp)\n",
    "\n",
    "tokenizer_loaded = load_tokenizer()\n",
    "\n",
    "chatbot = tf.keras.models.load_model(\n",
    "    hparams_loaded.save_model,\n",
    "    custom_objects={\n",
    "            \"PositionalEncoding\": model.PositionalEncoding,\n",
    "            \"MultiHeadAttentionLayer\": model.MultiHeadAttentionLayer,\n",
    "        },\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"Hello, my name is Omid . what about you?\",\n",
    "    \"how was your day\",\n",
    "    \"How old are you?\",\n",
    "    \"where have you been\",\n",
    "    \"do you like pizza?\",\n",
    "    \"my favourite color is blue\",\n",
    "    None, None, None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "HS5aVXLhHZ3A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8008,
     "status": "ok",
     "timestamp": 1667547853822,
     "user": {
      "displayName": "Omid Mollaei",
      "userId": "07035692833892294312"
     },
     "user_tz": -210
    },
    "id": "HS5aVXLhHZ3A",
    "outputId": "c2c96cb1-9404-4262-e681-d620585365eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-evaluating ...\n",
      "\n",
      "Input: Hello, my name is Omid . what about you?\n",
      "Output: i am getting out of here . if you do not mind .\n",
      "\n",
      "Input: how was your day\n",
      "Output: it is not a deal .\n",
      "\n",
      "Input: How old are you?\n",
      "Output: i am fine .\n",
      "\n",
      "Input: where have you been\n",
      "Output: i am going to see him .\n",
      "\n",
      "Input: do you like pizza?\n",
      "Output: yes , sir .\n",
      "\n",
      "Input: my favourite color is blue\n",
      "Output: i am not sure . i was so sure\n",
      "\n",
      "Input: i am not sure . i was so sure\n",
      "Output: i am sorry . i just wanted to talk to you .\n",
      "\n",
      "Input: i am sorry . i just wanted to talk to you .\n",
      "Output: i am sure it is okay .\n",
      "\n",
      "Input: i am sure it is okay .\n",
      "Output: you are a policeman .\n"
     ]
    }
   ],
   "source": [
    "evaluate(hparams_loaded, chatbot, tokenizer_loaded, sentences)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
